import os
import sys
import json
import asyncio
import yfinance as yf
import pandas as pd
import FinanceDataReader as fdr

from datetime import datetime, timedelta
from apscheduler.schedulers.blocking import BlockingScheduler

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- ì„œë¹„ìŠ¤ ë° ë¡œì§ ì„í¬íŠ¸ ---
from predict_rate_model import Predictor
from app.services.exchange_rate_crawler import get_detailed_exchange_rates

# --- ì„¤ì • ---
MODEL_DIRECTORY = "models/seq_120-pred_1-hidden_196-layers_2-batch_16-lr_0.0005-scaler_standard-tag_realGapSuperSimple_15years120days"
PREDICTION_CACHE_FILE = "prediction_cache.json"

# --- ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ) ---
if not os.path.exists(MODEL_DIRECTORY):
    raise FileNotFoundError(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{MODEL_DIRECTORY}'")
predictor = Predictor(model_dir=MODEL_DIRECTORY)
print("âœ… Predictorê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

async def run_and_cache_prediction_async():
    """
    ë¹„ë™ê¸° I/O(í¬ë¡¤ë§)ë¥¼ í¬í•¨í•˜ì—¬ AI ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹±í•©ë‹ˆë‹¤.
    """
    print(f"[{datetime.now()}] ğŸ¤– AI ì˜ˆì¸¡ ë° ìºì‹± ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. AI ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ê³¼ê±° + í˜„ì¬)
    seq_len = predictor.config['sequence_length'] # ëª¨ë¸ì´ í•„ìš”ë¡œ í•˜ëŠ” ë°ì´í„° ê¸¸ì´ (ì˜ˆ: 90)
    yesterday = datetime.now() - timedelta(days=1)
    start_date_fetch = yesterday - timedelta(days=seq_len * 2) # ë„‰ë„‰í•˜ê²Œ ì¡°íšŒ

    try:
        # â­ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ ìˆ˜ì • â­ï¸
        # 1.1. yfinanceì—ì„œ KRW=X ë°ì´í„° ë¡œë“œ
        df_inv = yf.download("KRW=X", start=start_date_fetch, end=yesterday)
        df_inv.columns = df_inv.columns.droplevel('Ticker')
        df_inv = df_inv[['Close']]
        df_inv.rename(columns={'Close': 'Inv_Close'}, inplace=True)
        df_inv.columns.name = None
        #print(df_inv.tail())
        #quit()
        ##df_inv = fdr.DataReader("USD/KRW", start_date_fetch, yesterday)
        ##df_inv.rename(columns={'Close': 'Inv_Close'}, inplace=True)

        # 1.2. ECOS í•œêµ­ì€í–‰ ê¸°ì¤€í™˜ìœ¨ ë°ì´í„° ë¡œë“œ
        df_ecos = fdr.DataReader('ECOS-KEYSTAT:K152', start_date_fetch, yesterday)
        # ECOS ë°ì´í„°ì˜ ì»¬ëŸ¼ ì´ë¦„ì€ ë‚ ì§œë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì„ íƒ
        df_ecos.rename(columns={df_ecos.columns[0]: 'ECOS_Close'}, inplace=True)
        
        # 1.3. ë‘ ë°ì´í„°í”„ë ˆì„ì„ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        df_merged = pd.merge(df_inv[['Inv_Close']], df_ecos[['ECOS_Close']], left_index=True, right_index=True, how='outer')
        # ECOS ë°ì´í„°ê°€ ìµœì‹  ë‚ ì§œì— ì—†ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ì´ì „ ê°’ìœ¼ë¡œ ì±„ì›€ (Forward Fill)
        df_merged.ffill(inplace=True)

        # 1.4. ì˜¤ëŠ˜ì˜ ì‹¤ì‹œê°„ ë°ì´í„° (í¬ë¡¤ëŸ¬)
        realtime_rates = await get_detailed_exchange_rates()
        today_rate_detail = next((r for r in realtime_rates if r.currency_code == 'USDKRW'), None)
        if not today_rate_detail:
            raise ValueError("ì‹¤ì‹œê°„ USD/KRW í™˜ìœ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        today_index = pd.to_datetime(datetime.now().strftime('%Y-%m-%d'))

        ####âœï¸ ì‹œê°„ëŒ€ë³„ë¡œ ì˜¤ëŠ˜ì 'Inv_Close', 'ECOS_Close' ë°ì´í„° ì–´ë–»ê²Œ ì±„ì›Œë„£ì–´ì•¼ í• ì§€ ë¡œì§ ê³ ë¯¼ í•„ìš”
        # ì˜¤ëŠ˜ ë°ì´í„°ì— Inv_Closeë§Œ ì¡´ì¬, ECOS_CloseëŠ” NaN
        df_today = pd.DataFrame({'Inv_Close': [today_rate_detail.rate], 'ECOS_Close': [today_rate_detail.rate]}, index=[today_index])
        print(df_today)
        # 1.5. ê³¼ê±° ë°ì´í„°ì™€ ì˜¤ëŠ˜ ë°ì´í„° í•©ì¹˜ê¸°
        df_combined = pd.concat([df_merged, df_today])
        # ë‹¤ì‹œ í•œë²ˆ Forward Fillì„ í†µí•´ ì˜¤ëŠ˜ì˜ ECOS_Close ê°’ì„ ì–´ì œ ê°’ìœ¼ë¡œ ì±„ì›€
        df_combined.ffill(inplace=True)

        # 1.6. 'diff' í”¼ì²˜ ê³„ì‚°
        df_combined['diff'] = df_combined['Inv_Close'] - df_combined['ECOS_Close']
        df_combined.dropna(inplace=True) # ê³„ì‚° í›„ NaNì´ ìˆì„ ê²½ìš° ì œê±°

        # 1.7. ëª¨ë¸ ì…ë ¥ì— í•„ìš”í•œ ìµœì¢… ë°ì´í„° ì„ íƒ
        input_df = df_combined.tail(seq_len)
        if len(input_df) < seq_len:
            print(f"âŒ ì „ì²˜ë¦¬ í›„ ì˜ˆì¸¡ì— í•„ìš”í•œ ë°ì´í„° ê¸¸ì´({seq_len})ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return

    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        return

    # 2. ì˜ˆì¸¡ ìˆ˜í–‰
    print("ğŸ“ˆ ëª¨ë¸ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    predicted_prices = predictor.predict(input_df)
    print(f"ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼ (raw): {predicted_prices}")

    # 3. ì˜ˆì¸¡ ê²°ê³¼ ê°€ê³µ ë° JSON íŒŒì¼ë¡œ ì €ì¥
    last_date = input_df.index[-1]

    # 3.1. API ì‘ë‹µì„ ìœ„í•´ ê³¼ê±° 30ì¼ì¹˜ ECOS ë§¤ë§¤ê¸°ì¤€ìœ¨ ë°ì´í„° ì¤€ë¹„
    df_historical_30d = df_combined[['ECOS_Close']].tail(30)
    
    historical_points = []
    for date, row in df_historical_30d.iterrows():
        historical_points.append({
            "date": date.strftime('%Y-%m-%d'),
            "rate": round(row['ECOS_Close'], 2),
            "is_prediction": False
        })
    
    # [ìˆ˜ì •ë¨] 3.2. Inv_Closeë¡œ ê°’ì„ ë®ì–´ì“°ëŠ” ë¡œì§ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.
    # ì´ì œ historical_pointsì˜ ë§ˆì§€ë§‰ ê°’ì€ í•­ìƒ ECOS_Closeì˜ ë§ˆì§€ë§‰ ê°’ì´ ë©ë‹ˆë‹¤.
    print(f"âœ… ê³¼ê±° ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ì§€ì ì€ ECOS_Close ê°’({historical_points[-1]['rate']:.2f})ì„ ìœ ì§€í•©ë‹ˆë‹¤.")

    # 3.3. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ historical_points ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    predictions_for_api = historical_points

    # [ìˆ˜ì •ë¨] 3.3.1. ê·¸ë˜í”„ ì—°ê²°ì„ ìœ„í•œ 'ë¸Œë¦¿ì§€' í¬ì¸íŠ¸ ì¶”ê°€
    # historical_pointsì— ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ë§ˆì§€ë§‰ í¬ì¸íŠ¸ë¥¼ ë³µì‚¬í•˜ì—¬ is_prediction=trueë¡œ ì„¤ì •í•œ í›„ ì¶”ê°€í•©ë‹ˆë‹¤.
    if historical_points:
        bridge_point = historical_points[-1].copy()
        bridge_point['is_prediction'] = True
        predictions_for_api.append(bridge_point)
        print(f"âœ… ê·¸ë˜í”„ ì—°ê²°ì„ ìœ„í•´ ë¸Œë¦¿ì§€ í¬ì¸íŠ¸({bridge_point['date']})ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    
    if predicted_prices.size > 0:
        price = predicted_prices[0]
        pred_date = last_date + timedelta(days=1)
        predictions_for_api.append({
            "date": pred_date.strftime('%Y-%m-%d'),
            "rate": round(float(price), 2),
            "is_prediction": True
        })
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼({pred_date.strftime('%Y-%m-%d')})ë¥¼ ê³¼ê±° ë°ì´í„°ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì˜ˆì¸¡ ê²°ê³¼ê°€ ë¹„ì–´ìˆì–´ ì¶”ê°€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
    # 3.4. ìµœì¢… ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    cache_data = {
        "updated_at": datetime.now().isoformat(),
        "predictions": predictions_for_api # ì´ì œ ì´ ë³€ìˆ˜ëŠ” ê³¼ê±° ë°ì´í„° + ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ëª¨ë‘ í¬í•¨
    }

    with open(PREDICTION_CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ! '{PREDICTION_CACHE_FILE}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def run_prediction_job():
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë™ê¸° ë˜í¼ í•¨ìˆ˜"""
    asyncio.run(run_and_cache_prediction_async())

# --- ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ---
sched = BlockingScheduler()
@sched.scheduled_job('cron', hour=23, minute=30) # ë§¤ì¼ ë°¤ 11ì‹œ 30ë¶„ì— ì‹¤í–‰
def scheduled_job():
    run_prediction_job()

if __name__ == "__main__":
    print("ğŸš€ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ˆê¸° ì˜ˆì¸¡ì„ ë¨¼ì € ì‹¤í–‰í•©ë‹ˆë‹¤.")
    run_prediction_job()
    sched.start()
